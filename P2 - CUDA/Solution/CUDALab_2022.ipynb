{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bx2JSyCHfCq"
      },
      "source": [
        "# Lab 2: Color conversion in CUDA\n",
        "\n",
        "Please, follow the order of the sections, complete the code, and build a report that encapsulates everything you have learnt and understood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKv88s8QsneN"
      },
      "source": [
        "## Previous information\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4B3Aenesmcf"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Following the previous laboratory based on OpenMP, now we are going to work on the same simple algorithm, in CUDA. The goal of this lab is to learn the basics of CUDA kernels, and CUDA Host code.\n",
        "\n",
        "This new ipython notebook format, will make things easyer since explanation and code will coexist in the same document, and it will be very clear what you need to do.\n",
        "\n",
        "Additionally, having the Colab platform available with NVIDIA GPU's makes it simpler than ever. You can learn CUDA from any system, Mac, Windows, Linux, and any hardware, Intel, AMD, NVIDIA, and possibliy even ARM on tablets. You only need a web browser compatible with Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6wwlQsssj4b"
      },
      "source": [
        "\n",
        "### Structure of the Lab\n",
        "You already know the algorithm from the previous lab, but you may not be familiar with this environment.\n",
        "\n",
        "First we will try to understand a bit this environment, and then we will explain section by section what you have to do. \n",
        "\n",
        "You will have to complete code  and perform experiments in each of the sections. Then, **you will be asked to comment the results in a separated report**:, just as you needed to do for the first lab. Use tables and figures that support both the results you collected and the arguments you make to justify the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yqTZrbsemC"
      },
      "source": [
        "\n",
        "### The collab environment for CUDA\n",
        "\n",
        "First of all, you should know that we are executing an iPython notebook in a Google Colab session. The notebook is preconfigured with the type of execution environment we need, a GPU execution environment. But the files we generate, and the pluggins we install or enable, reside on the Google Colab session. All this will be removed when we exit the session either manually or implicitly by closing the broser.\n",
        "\n",
        "In order to have a GPU available when creating a new notebook, you only have to select the execution environment.\n",
        "In Spanish, go to \"Entorno de ejecución->Cambiar tipo de entorno de ejecución\" and then select GPU.\n",
        "\n",
        "But as we already mentioned, this notebook is already configured, so you don't need to do it again.\n",
        "\n",
        "Now, the first thing we will see is that we have the nvcc compiler. We can call many bash commands with ! as the first character, in a code block. Next you will find a code block with a call to nvcc (the nvidia CUDA compiler) with a flag that asks for the compiler version.\n",
        "\n",
        "Click on the block and then a play button will appear on the left. Click on the play button. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nqPtNfIs3Zi"
      },
      "source": [
        "## Getting started..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp5rooWw0U_t"
      },
      "source": [
        "First we will check the CUDA compiler version we have:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNXYi-1xD-Zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b896672-4419-4daa-aaa0-af67c6087562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rhjdqSImo2"
      },
      "source": [
        "You can also execute it by placing the cursor inside the code block and pressing Shift+Enter\n",
        "\n",
        "Next you need to install a pluggin, that does not come with the notebook. In the following code block you have the code line to be executed. You will have to execute this code every time you open the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l1NOZW5ET_p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f12cee0-d682-49bc-a353-7c11512e9fb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-z9ts9bbm\n",
            "  Running command git clone -q https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-z9ts9bbm\n",
            "directory /content/src already exists\n",
            "Out bin /content/result.out\n"
          ]
        }
      ],
      "source": [
        "#!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMdtAyAFz4L-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aadc0ae8-957a-49fe-9167-c3cff789bc93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local\n"
          ]
        }
      ],
      "source": [
        "# DO FOR GPU K80\n",
        "%cd /usr/local/\n",
        "!rm -rf cuda\n",
        "!ln -s /usr/local/cuda-10.1 /usr/local/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuMz7OOYJVfk"
      },
      "source": [
        "Now, you can compile and execute CUDA code, just by puting the same code you would put ina .cu file, just by adding %%cu as the first line.\n",
        "\n",
        "Next, you have a code example. Try it! Read the comments to help you understand it. It will be very useful for the tasks you have to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x03T7kcmEgu4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54bfdc3e-4aac-45b5-a31b-9f0f3c6839db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result of multiplying 3 * 5 is 15\n",
            "\n",
            "Device Number: 0\n",
            "  Device name: Tesla K80\n",
            "  Memory Clock Rate (KHz): 2505000\n",
            "  Memory Bus Width (bits): 384\n",
            "  Peak Memory Bandwidth (GB/s): 240.480000\n",
            "\n",
            "Num devices 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// Function to print the cuda errors\n",
        "void cuCheck(cudaError_t err) {\n",
        "    if(err!=cudaSuccess) {\n",
        "          printf(\"CUDA error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "}\n",
        "\n",
        "// This macros help with capturing the possible cuda errors and printing\n",
        "// the error name to help the developer.\n",
        "// Kernels are always asynchronous to the Host, so they don't return\n",
        "// any value. Then to see if any error has happened, you should call cudaGetLastError\n",
        "// and pass the result to cuCheck()\n",
        "// Use the macros instead, to make it simpler.\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())\n",
        "\n",
        "// Device code or Kernel\n",
        "__global__ void mult(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    \n",
        "    // Host variables a & b\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "\n",
        "    // Host variable that will store a Device pointer wich we can later on \n",
        "    // download to the Host.\n",
        "    // As this variable will contain pointers that are only valid in\n",
        "    // the Device (the GPU) it will be invalid to access them from\n",
        "    // Host code. We only can use them in the right cuda API calls\n",
        "    // or inside a cuda Kernel.\n",
        "    // So in this part of the code you won't be able to do d_c[0], for instance\n",
        "    int *d_c;\n",
        "\n",
        "    // Size of the data contained in variables a, b and c.\n",
        "    int dataSize = sizeof(int);\n",
        "\n",
        "    // Reserve Device memory using the cuda API\n",
        "    // cudaMalloc will place a Device pointer inside d_c.\n",
        "    CU_CHECK(\n",
        "        cudaMalloc((void **)&d_c, dataSize)\n",
        "    );\n",
        "\n",
        "    // Launch mult() kernel on GPU\n",
        "    // Notice that a and b are not pointers. Therefore the kernel call will\n",
        "    // copy their values but the variables inside the kernel will not be the same.\n",
        "    // If we modify a and b inside the kernel, it will not change a and b in this\n",
        "    // Host code. This, indeed is the same behavior as any C/C++ function call.\n",
        "    // In the case of d_c, it will copy the pointer contained in d_c, \n",
        "    // so we will be able to modify the contents of d_c from the kernel. But to read \n",
        "    // them from this Host code, we will have to do something else.\n",
        "    mult<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "\n",
        "    CU_CHECK(\n",
        "        // Copy result back to host\n",
        "        cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost)\n",
        "    );\n",
        "\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\\n\",a,b,h_c);\n",
        "\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceCount(&numDevs)\n",
        "    );\n",
        "\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(\n",
        "        cudaGetDeviceProperties(&prop, 0)\n",
        "    );\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    // Cleanup\n",
        "    CU_CHECK(\n",
        "      cudaFree(d_c)\n",
        "    );\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y-tsavm1nzC"
      },
      "source": [
        "> If your GPU is other than Tesla K80, tell the teacher please."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajyZJvBCZR7p"
      },
      "source": [
        "Ok, cool! But what if I want to have some code in a .h file, the cuda kernels in an other .h file, and include both so that I can reuse code?\n",
        "\n",
        "Ok, let's try to put the macros and cuCheck function in a .h file, the kernel in an other .h file and the rest in a .cu file, and compile and execute everything. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlObGBaEZ5M7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "77dd4773-75bd-495b-ab87-4f17922b4d0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/utils.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "%%cuda --name utils.h\n",
        "#include <iostream>\n",
        "\n",
        "void cuCheck(cudaError_t err, const std::string message = \"CUDA error:\") {\n",
        "  if(err!=cudaSuccess) {\n",
        "    std::cout << message << \" ERROR \" << cudaGetErrorString(err) << std::endl;\n",
        "  }\n",
        "}\n",
        "#define CU_CHECK(a) cuCheck(a)\n",
        "#define CU_CHECK2(a, b) cuCheck(a, b)\n",
        "#define CU_CHECK_LAST_ERROR cuCheck(cudaGetLastError())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEoYz9uug7om",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eb465285-9240-4c6d-fb85-ee0a89bc1f44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/kernels.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "%%cuda --name kernels.h\n",
        "__global__ void mult(int a, int b, int* __restrict d_c) {\n",
        "    *d_c = a * b;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdHjgZRHlSGe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e91b15a-f735-4873-cd82-34da8d0c3440"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla K80\n",
            "  Memory Clock Rate (KHz): 2505000\n",
            "  Memory Bus Width (bits): 384\n",
            "  Peak Memory Bandwidth (GB/s): 240.480000\n",
            "\n",
            "Num devices 1\n",
            "Result of multiplying 3 * 5 is 15\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/kernels.h\"\n",
        "\n",
        "// Host code\n",
        "int main() {\n",
        "    int a = 3, b = 5, h_c = 0;\n",
        "    int *d_c;\n",
        "    int dataSize = sizeof(int);\n",
        "    CU_CHECK(cudaMalloc((void **)&d_c, dataSize));\n",
        "    mult<<<1,1>>>(a, b, d_c);\n",
        "    CU_CHECK_LAST_ERROR;\n",
        "    CU_CHECK(cudaMemcpy(&h_c, d_c, dataSize, cudaMemcpyDeviceToHost));\n",
        "    int numDevs=0;\n",
        "    CU_CHECK(cudaGetDeviceCount(&numDevs));\n",
        "    cudaDeviceProp prop;\n",
        "    CU_CHECK(cudaGetDeviceProperties(&prop, 0));\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\",\n",
        "          prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\",\n",
        "          prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\",\n",
        "          2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "    printf(\"Num devices %d\\n\", numDevs);\n",
        "    printf(\"Result of multiplying %d * %d is %d\\n\",a,b,h_c);\n",
        "    CU_CHECK(cudaFree(d_c));\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8rWgfJ8hN3m"
      },
      "source": [
        "VERY IMPORTANT!!! On each Colab session, the GPU that Google Colab provides can be different. Take it into account when you perform experiments, so that you compare results for the same GPU.\n",
        "\n",
        "If you have to repeat all the experiments, well, it's not that hard, just click play in all the code blocks one by one.\n",
        "\n",
        "Great!! Now we can start the lab :-D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX0Yv50Wh4nY"
      },
      "source": [
        "##Section 1:\n",
        "\n",
        "Try to complete the following code, and make it compile. Remember that you have some slides and documents, and the CUDA API specification in the following link: https://docs.nvidia.com/cuda/cuda-runtime-api/index.html\n",
        "\n",
        "Also, you can search in Google, things like \"How to allocate CUDA memory\". And so on. Be brave! Is not so difficult.\n",
        "\n",
        "### First, complete the allocation functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6Bj54YdVsjI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d422ffb7-6bb6-497f-8921-d4fa58689536"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpy(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemset(d_rgba, 0, width*height*sizeof(uchar4)), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first task from the code above is to allocate the d_rgba data. We have accomplished it by replicating the code from the function above. However, here we change the data type for the size as it changes from uchar3 to uchar4.\n",
        "\n",
        "The second part is to free the data structure that we have allocated, otherwise we could run into memory leak problems. In this case we only have to free the d_rgba variable making a cudaFree call."
      ],
      "metadata": {
        "id": "EORSbl8eXdb1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s30mTBEIWcX6"
      },
      "source": [
        "### When completed, test that they work with this small main function. If you execute it without completing the previous code, it will show some errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tup9fxlwWl64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99651715-9626-4cc8-a139-56087f962e6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <cuda.h>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "\n",
        "#define WIDTH 10\n",
        "#define HEIGHT 10\n",
        "\n",
        "int main() {\n",
        "\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSfdiwiHZKPT"
      },
      "source": [
        "### Ok, now that we have the allocation, copy and free functions implemented, let's continue with the CPU function that will check the results. This one it's already implemented, you only need to click play to have it available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fywcbihaBaR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8842344c-ff38-43ee-bbee-fd4de8a33b5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/check_results.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "%%cuda --name check_results.h\n",
        "bool checkResults(uchar4* rgba, uchar3* brg, int size) {\n",
        "  bool correct = true;\n",
        "  for (int i=0; i < size; ++i) {\n",
        "    // In case you want to see actual values\n",
        "    if (i==3) {\n",
        "      unsigned char x, y, z, w;\n",
        "      x = rgba[i].x;\n",
        "      y = rgba[i].y;\n",
        "      z = rgba[i].z;\n",
        "      w = rgba[i].w;\n",
        "      std::cout << \"First position x=\" << (unsigned int)x << \" y=\" << (unsigned int)y << \" z=\" << (unsigned int)z << \" w=\" << (unsigned int)w << std::endl;\n",
        "    }\n",
        "    correct &= rgba[i].x == brg[i].y;\n",
        "    correct &= rgba[i].y == brg[i].z;\n",
        "    correct &= rgba[i].z == brg[i].x;\n",
        "    correct &= rgba[i].w == 255;\n",
        "  }\n",
        "  return correct;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl-YagCWaZOS"
      },
      "source": [
        "### Now the interesting part, the kernel and the code to configure and launch it. The kernel it's almost exactly the same code as the OpenMP lab, only we replaced the for loops with something that you need to implement.\n",
        "\n",
        "Remember, that we have threads with indexes. This indexes are used to tell each CUDA thread, which data do they have to read or write.\n",
        "\n",
        "The structs that contain those indexes are in the documentation you have available in campusvirtual. Please check the docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et3Bww3PbMgw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "76517277-e1f3-45e9-daf7-8abb6a8e4acc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "// BIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = blockIdx.x * blockDim.x + threadIdx.x; //Use the thread id and block id's to compute x \n",
        "  int y = blockIdx.y * blockDim.y + threadIdx.y; //Use the thread id and block id's to compute y\n",
        "\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width && y < height) {\t\n",
        "\t    rgba[width * y + x].x = brg[width * y + x].y;\n",
        "\t    rgba[width * y + x].y = brg[width * y + x].z;\n",
        "\t    rgba[width * y + x].z = brg[width * y + x].x;\n",
        "\t    rgba[width * y + x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(256, 4, 1);\n",
        "  dim3 grid(ceil(width/(float)block.x),ceil(height/(float)block.y) , 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "\n",
        "  CU_CHECK2(cudaFree(d_secondData), \"cudaFree in executeKernelconvertBRG2RGBA\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The changes done in this code cell are in the lines 4 and 5. Here we are getting the row and column number by computing on which thread are we located. To do that, we take the number of the block that we are in and then we multiply it by the number of threads that a block can contain. After having that number if we sum the thread id that we have we will exactly know where are we located."
      ],
      "metadata": {
        "id": "aQyhoIGfY-Tw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2Ty-GmcSY_"
      },
      "source": [
        "### MAIN EXPERIMENT \n",
        "Try all the previous code, with the following main. If you did not finish all the previous code, this file will show some execution errors.\n",
        "\n",
        "The code is divided in two parts, one to define the parameters of the experiment and the other one is the main function with the experiment it self.\n",
        "\n",
        "The experiment is the code that creates a BRG image in CPU, allocates GPU memory, copies the BRG image to GPU memory, and executes a GPU kernel to convert the BRG image into a RGBA image. The output of the kernel is another GPU pointer, so after the kernel execution, we have to copy back the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwcU52z8B--N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a30aab00-9383-4512-cd1c-72cbe96245ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment_settings.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "%%cuda --name experiment_settings.h\n",
        "#pragma once\n",
        "#define WIDTH 3840\n",
        "#define HEIGHT 2160\n",
        "#define EXPERIMENT_ITERATIONS 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT9ep3h_ijOO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3aa9baed-ebe5-44e3-f214-3d67533444df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = (HEIGHT/3) * WIDTH;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "  \n",
        "  // Prepare and copy data to GPU\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS);\n",
        "\n",
        "  // Copy data back from GPU to CPU, without streams\n",
        "  // We make a call to cudaMemcpy to pass the data to the CPU. We take into consideration the size of the data.\n",
        "  CU_CHECK2(cudaMemcpy(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost), \"Cuda memcpy Device to Host: \");\n",
        "    \n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5RA9zDI7Z0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a336f3-5a4b-4a83-f6b5-623bc4cf0818"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convertBRG2RGBA time for 1 iterations = 1526us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqpbCcVdK83"
      },
      "source": [
        "##Section 2:\n",
        "Implement a version of the kernel and launcher that uses a one dimensional cuda GRID. That is, there is no more x and y, only x.\n",
        "\n",
        "Modify the code below, click play, and then click play in the Main Experiment block, in Section 1.\n",
        "\n",
        "Try different values of BLOCK_SIZE.\n",
        "\n",
        "Check if there is any execution time improvement, compared to Section 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLPY7trNd1SG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ab0465f8-4e14-4278-80e5-5e0ab077b9c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = blockIdx.x * blockDim.x + threadIdx.x; // Use the thread id and block id's to compute x \n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    rgba[x].x = brg[x].y;\n",
        "\t    rgba[x].y = brg[x].z;\n",
        "\t    rgba[x].z = brg[x].x;\n",
        "\t    rgba[x].w = 255;\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just as in the section 1 we can locate where we are at by performing the calculation with the block ID, block dimension and thread ID. That way we can find which X do we have."
      ],
      "metadata": {
        "id": "Y7s2XUcrZ4bp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozsLq2rt__xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51472945-ceb8-4154-84a1-958a07a25287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convertBRG2RGBA time for 1 iterations = 1294us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKiFjB66Rp_j"
      },
      "source": [
        "Change the WIDTH to 3839 and reexcecute section 1 and section 2. Compare the execution times and explain the difference.\n",
        "\n",
        "Normal execution:\n",
        "\n",
        "\n",
        "*   Section 1: 1374us\n",
        "*   Section 2: 1351us\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Width modified:\n",
        "\n",
        "\n",
        "*   Section 1: 1348us\n",
        "*   Section 2: 1390us\n",
        "\n",
        "We don't see a clear trend when comparing both sections, as with the first section the time consumed has decrease while in the second section it has increased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExMCHf_RD3Wd"
      },
      "source": [
        "Change the experiment settings, by executing more iterations and compare the unidimensional kernel with the bidimensional kernel.\n",
        "\n",
        "Comment the results in the report.\n",
        "\n",
        "With one iteration:\n",
        "*   Section 1: 1374us\n",
        "*   Section 2: 1351us\n",
        "\n",
        "With ten iterations:\n",
        "*   Section 1: 8170us\n",
        "*   Section 2: 8156us\n",
        "\n",
        "Wit a hundred iterations:\n",
        "*   Section 1: 76217us\n",
        "*   Section 2: 76248us\n",
        "\n",
        "We can clearly see that if we increase the number of iterations the time consumed increases. However, we can also see that the difference in all of the cases are minimal between the first and the second experiments. This can be related to the limited capabilities that are provided by the GPUs that Google is lending us.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll compare with different block sizes:\n",
        "\n",
        "\n",
        "*   128: 1380us\n",
        "*   256(default): 1415us\n",
        "*   512: 1406us\n",
        "*   1024: 1428us\n",
        "\n",
        "From these results we can conclude that the best time would be obtained by using a BLOCK_SIZE of 128, yet the results vary so little that we should keep an eye when dealing with bigger data sizes."
      ],
      "metadata": {
        "id": "-zwOHxN7f1qj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3gBy530ektk"
      },
      "source": [
        "## Section 3:\n",
        "\n",
        "Starting from Section 2, (use the best BLOCK_SIZE you found) try to optimize the memory accesses in some way, without using shared memory.\n",
        "\n",
        "Comment in the report which memory access problems you observe. Are the memory accesses aligned, and therfore coalesced?\n",
        "\n",
        "Remember that opposite to what the CPU compilers do, the nvcc compiler does not optimize the memory accesses in structs\n",
        "\n",
        "Remember also that GPU memory is organized in blocks of 4 bytes, and any array based on data elements that are not multiple of 2, will not be alligned. To be coalesced (specially in old architectures), it also has to be multiple of 4."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answering the question of memory access problems, yes in the Section 2 we have made too many accesses to the shared memory.\n",
        "\n",
        "About the coalesced memory readings, unlike with OMP we don't have the advantage of having a \"function\" that let's us align the data that we were using. So, as we're using 3 bytes per pixel and the memory structure goes in a 4 by 4 byte way, we will have to make 2 accesses to memory most of the times."
      ],
      "metadata": {
        "id": "50nqGYm4cQhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmP4PDZ-e4lG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d57218c0-217c-4ca7-e3f2-408e43e674d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL BETTER MEMORY ACCESS\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int x = blockIdx.x * blockDim.x + threadIdx.x; //Use the same code as in section 2 in this line\n",
        "  \n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (x < width * height) {\t\n",
        "\t    // do something different here, to optimize memory accesses\n",
        "      uchar3 brgInPrivateMemory = brg[x]; // Storing the whole array position into the private memory\n",
        "      rgba[x] = make_uchar4(brgInPrivateMemory.y, brgInPrivateMemory.z, brgInPrivateMemory.x, 255); // Assigning back from the temporal memory to the uchar4\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we will first do in the protection against segmentation fault is to pass the array position into a temporal variable, with this variable we will avoid a triple access to memory in the next line. Now, in the following line we assing the data to each position of the uchar4 from the data that we have retrieved in the temporal variable."
      ],
      "metadata": {
        "id": "xqAUvA0-acQZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMDGsQjPAkIv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80784f99-0c05-45dd-f5bf-51474fbe8bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convertBRG2RGBA time for 1 iterations = 1177us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPECp0T6IU0Z"
      },
      "source": [
        "##Section 4:\n",
        "Now optimize the GPU memory accesses so that each thread always reads at least one element of 4 bytes. Use shared memory to that end.\n",
        "\n",
        "Look at the PDF Lab2CUDA in the campus, an read the last two pages. There you have a graphical explanation of the kernel issues. For this section you only need to understand the first figure.\n",
        "\n",
        "About shared memory: we will refresh some concepts.\n",
        "\n",
        "Shared memory, is a kind of memory that is visible only by the cuda threads of a thread block. Cuda threads from different thread blocks can not see the shared memory of other threadblocks.\n",
        "\n",
        "Shared memory is a limited resource. Depending on the GPU model, you may have from 32KB to 64KB of shared memory. Additionally, this memory is not used only by one threadblock. It is partitioned in as many independent blocks as thread blocks can execute in a single Streaming Multiprocessor (check the documentation if you don't know what a SM is). \n",
        "\n",
        "So when you are defining the amount of shared memory you want, you are defining the amount of memory, every thread block will have available.\n",
        "\n",
        "If you reserve 64KB of shared memory, in a GPU that has this capacity, only one thread block will execute on each SM, which is super slow. Each SM can concurrently execute from 8 to 32 thread blocks. For the best performance, you usually want the greatest amount of thread blocks active on each SM.\n",
        "\n",
        "Therefore, you what to use the least shared memory possible, and only use it when it has clear benefits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKRs22QeIvwj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92abcb71-15b9-4b2d-9b48-ad2636f610b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different values of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = N_ELEMS_3_4_TBLOCK * blockIdx.x + threadIdx.x; // Use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = blockIdx.x * blockDim.x + threadIdx.x; // Recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the position we use the same formula as in sections 1 and 2. But for this specific case we left the block dimension and we used instead N_ELEMS_3_4_TBLOCK to \"say that each block has elements of 4 bytes\".\n",
        "\n",
        "Later on we have to recalculate the position without the N_ELEMS_3_4_TBLOCK as we have to write the results."
      ],
      "metadata": {
        "id": "_fuS57skbNbU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E_ErYqLAtVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef11067e-e0f0-417b-c9f4-c9df1460066e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convertBRG2RGBA time for 1 iterations = 1205us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW8E-e-DVXk1"
      },
      "source": [
        "Coment in the report how the time execution is changing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MazFSZvFO9lT"
      },
      "source": [
        "##Section 5:\n",
        "\n",
        "Change all the host code necessary, to use cuda streams. Here you have an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EvgoMT5PVYn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b8782b-0253-4c72-f68a-55971d2b449f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished and results are correct.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "\n",
        "__global__ void square(int* d_input, int* d_output) {\n",
        "    int x = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "    int val = d_input[x];\n",
        "    // We exploit the temporal locality of the value stored in d_output[x]\n",
        "    d_output[x] = val*val;\n",
        "}\n",
        "\n",
        "static const size_t dataSize = sizeof(int)*1024;\n",
        "\n",
        "int main() {\n",
        "    \n",
        "    int *h_input, *h_output;\n",
        "    h_input = (int*)malloc(dataSize);\n",
        "    h_output = (int*)malloc(dataSize);\n",
        "\n",
        "    for (int i=0; i<1024; ++i) h_input[i]=i;\n",
        "\n",
        "    int *d_input, *d_output;\n",
        "    cudaMalloc(&d_input, dataSize);\n",
        "    cudaMalloc(&d_output, dataSize);\n",
        "\n",
        "    cudaStream_t stream;\n",
        "    cudaStreamCreate(&stream);\n",
        "\n",
        "    dim3 block(512);\n",
        "    dim3 grid(2);\n",
        "\n",
        "    // The CPU thread does not wait that any of the following actions finish\n",
        "    // It only asks the GPU to do the copies and the kernel and continues\n",
        "    cudaMemcpyAsync(d_input, h_input, dataSize, cudaMemcpyHostToDevice, stream);\n",
        "    square<<<grid, block, 0, stream>>>(d_input, d_output);\n",
        "    cudaMemcpyAsync(h_output, d_output, dataSize, cudaMemcpyDeviceToHost, stream);\n",
        "\n",
        "    // Here, we wait for all the orders enqueued in stream, to finish.\n",
        "    cudaStreamSynchronize(stream);\n",
        "\n",
        "    bool correct = true;\n",
        "    for (int i=0; i<1024; ++i) correct &= h_output[i] == i*i;\n",
        "\n",
        "    std::cout << \"Finished and results are \" << (correct ? \"correct.\" : \"not correct.\") << std::endl;\n",
        "\n",
        "    cudaStreamDestroy(stream);\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    free(h_input);\n",
        "    free(h_output);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4VK6ylPWaEY"
      },
      "source": [
        "Modify this code, to use streams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAn_ggnrbAWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9c65d8c-4d90-4dd9-cd84-a37ac28142e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/cuda_launcher.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "%%cuda --name cuda_launcher.h\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "// Try different values of BLOCK_SIZE\n",
        "#define BLOCK_SIZE 256\n",
        "\n",
        "// Number of 4 byte elements that we can make out of BLOCK_SIZE elements of 3 bytes\n",
        "#define N_ELEMS_3_4_TBLOCK (BLOCK_SIZE * 3)/4\n",
        "#define N_ELEMS_3_4_IMAGE (WIDTH*HEIGHT * 3)/4\n",
        "\n",
        "// UNIDIMENSIONAL KERNEL SHARED MEMORY\n",
        "__global__ void convertBRG2RGBA(uchar3 *brg, uchar4* rgba, int width, int height) {\n",
        "  int position = N_ELEMS_3_4_TBLOCK * blockIdx.x + threadIdx.x; // Use N_ELEMS_3_4_TBLOCK to compute the position of each thread when we read brg as if it had elements of 4 bytes\n",
        "  __shared__ uchar4 bgrShared[N_ELEMS_3_4_TBLOCK];\n",
        "  \n",
        "  if(threadIdx.x < N_ELEMS_3_4_TBLOCK && position < N_ELEMS_3_4_IMAGE) {\n",
        "      uchar4* temp = reinterpret_cast<uchar4*>(brg);\n",
        "      bgrShared[threadIdx.x] = temp[position];\n",
        "  }\n",
        "\n",
        "  __syncthreads();\n",
        "  \n",
        "  position = blockDim.x * blockIdx.x + threadIdx.x; // Recompute position without N_ELEMS_3_4_TBLOCK to write the results\n",
        "\t// Protection to avoid segmentation fault\n",
        "\tif (position < width*height) {\t\n",
        "        uchar3 local = reinterpret_cast<uchar3*>(bgrShared)[threadIdx.x];\n",
        "        rgba[position] = make_uchar4(local.y,local.z,local.x,255);\n",
        "\t}\n",
        "}\n",
        "\n",
        "void executeKernelconvertBRG2RGBA(int width, int height, uchar3* d_brg, uchar4* d_rgba, int numIters, cudaStream_t stream=0) {\n",
        "  // Execute the GPU kernel\n",
        "  dim3 block(BLOCK_SIZE, 1, 1);\n",
        "  dim3 grid(ceil(width*height/(float)block.x), 1, 1);\n",
        "\n",
        "  // A trick to avoid some undesired optimizations, that will not happen in real applications\n",
        "  uchar3* d_dataVector[2];\n",
        "  uchar3* d_secondData;\n",
        "  CU_CHECK2(cudaMalloc(&d_secondData, width * height * sizeof(uchar3)), \"cudaMalloc in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  d_dataVector[0] = d_brg;\n",
        "  d_dataVector[1] = d_secondData;\n",
        "\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_secondData, d_brg, width * height * sizeof(uchar3), cudaMemcpyDeviceToDevice, stream), \"cudaMemcpyAsync in executeKernelconvertBRG2RGBA\");\n",
        "\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  for (int i=0; i<numIters; ++i) {\n",
        "    convertBRG2RGBA<<<grid, block, 0, stream>>>(d_dataVector[i%2], d_rgba, width, height);\n",
        "  }\n",
        "  CU_CHECK2(cudaDeviceSynchronize(), \"cudaDeviceSynchronize:\");\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count();\n",
        "  std::cout << \"convertBRG2RGBA time for \" << numIters << \" iterations = \"<< duration << \"us\" << std::endl;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yz3kVFkrWOlB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a4e2e32c-ac02-4919-fe20-464693e10671"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/memory_functions.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "%%cuda --name memory_functions.h\n",
        "void allocGPUData(int width, int height, uchar3** d_brg, uchar4** d_rgba){\n",
        "  // Alloc gpu pointers\n",
        "  CU_CHECK2(cudaMalloc(d_brg, sizeof(uchar3)*width*height), \"Alloc d_brg:\");\n",
        "  // Can you finish this one? Replace cudaSucces with the proper cuda API call\n",
        "  CU_CHECK2(cudaMalloc(d_rgba, sizeof(uchar4)*width*height), \"Alloc d_rgba:\");\n",
        "}\n",
        "void copyAndInitializeGPUData(int width, int height, uchar3* h_brg, uchar3* d_brg, uchar4* d_rgba, cudaStream_t stream=0) {\n",
        "  // Copy data to GPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(d_brg, h_brg, width*height*sizeof(uchar3), cudaMemcpyHostToDevice, stream), \"Copy h_brg to d_brg:\");\n",
        "  // Init output buffer to 0\n",
        "  CU_CHECK2(cudaMemsetAsync(d_rgba, 0, width*height*sizeof(uchar4), stream), \"Memset d_rgba:\");\n",
        "}\n",
        "void freeCUDAPointers(uchar3* d_brg, uchar4* d_rgba) {\n",
        "  // Free cuda pointers. Replace the cudaErrorInvalidValue flag\n",
        "  // with the proper cuda API call, to free the GPU pointers\n",
        "  CU_CHECK2(cudaFree(d_brg), \"Cuda free d_bgr:\");\n",
        "  CU_CHECK2(cudaFree(d_rgba), \"Cuda free d_rgba:\");\n",
        "  // Clean GPU device\n",
        "  CU_CHECK2(cudaDeviceReset(), \"Cuda device reset:\");\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the changes that we made to support the streams are on the copyAndInitializeGPUData function. Here we used cudaMemsetAsync to use the stream."
      ],
      "metadata": {
        "id": "BJdnRolnchop"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymPFWGqkWew-"
      },
      "source": [
        "And also modify this code, so that mem copies from CPU to GPU and from GPU to CPU use an stream, and are not blocking.\n",
        "\n",
        "Additionally, add a chrono between the first memcpy (included) and the cudaStreamSynchronize. This is the time you will have to compare.\n",
        "\n",
        "Follow the indications in the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih1ADZpjWeFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e07df112-5b18-47a0-9937-dbce6f0e5932"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'File written in /content/src/experiment.h'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "%%cuda --name experiment.h\n",
        "#include <cuda.h>\n",
        "#include <chrono>\n",
        "#include \"/content/src/utils.h\"\n",
        "#include \"/content/src/memory_functions.h\"\n",
        "#include \"/content/src/check_results.h\"\n",
        "#include \"/content/src/cuda_launcher.h\"\n",
        "#include \"/content/src/experiment_settings.h\"\n",
        "\n",
        "void executeExperiment() {\n",
        "  uchar3 *h_brg, *d_brg;\n",
        "  uchar4 *h_rgba, *d_rgba;\n",
        "\n",
        "  int bar_widht = HEIGHT/3;\n",
        "\n",
        "  // Alloc and generate BRG bars.\n",
        "  h_brg = (uchar3*)malloc(sizeof(uchar3)*WIDTH*HEIGHT);\n",
        "  for (int i=0; i < WIDTH * HEIGHT; ++i) {\n",
        "    if (i < bar_widht) {\n",
        "      uchar3 temp = {255, 0, 0};\n",
        "      h_brg[i] = temp; \n",
        "    } else if (i < bar_widht*2) {\n",
        "      uchar3 temp = {0, 255, 0};\n",
        "      h_brg[i] = temp;\n",
        "    } else { \n",
        "      uchar3 temp = {0, 0, 255};\n",
        "      h_brg[i] = temp;\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Alloc RGBA pointers\n",
        "  h_rgba = (uchar4*)malloc(sizeof(uchar4)*WIDTH*HEIGHT);\n",
        "\n",
        "  // Alloc gpu pointers\n",
        "  allocGPUData(WIDTH, HEIGHT, &d_brg, &d_rgba);\n",
        "\n",
        "  // Create here cuda stream\n",
        "  cudaStream_t stream;\n",
        "  cudaStreamCreateWithFlags(&stream, cudaStreamNonBlocking); // Added non-blocking flag\n",
        "  \n",
        "  // Start measuring time here\n",
        "  auto t1 = std::chrono::high_resolution_clock::now();\n",
        "  copyAndInitializeGPUData(WIDTH, HEIGHT, h_brg, d_brg, d_rgba, stream);\n",
        "\n",
        "  // Execute the GPU kernel\n",
        "  executeKernelconvertBRG2RGBA(WIDTH, HEIGHT, d_brg, d_rgba, EXPERIMENT_ITERATIONS, stream);\n",
        "\n",
        "  // Copy data back from GPU to CPU\n",
        "  CU_CHECK2(cudaMemcpyAsync(h_rgba, d_rgba, sizeof(uchar4)*WIDTH*HEIGHT, cudaMemcpyDeviceToHost, stream), \"Cuda memcpy Device to Host: \");\n",
        "\n",
        "  // Synchronize the stream here\n",
        "  cudaStreamSynchronize(stream);\n",
        "\n",
        "  // Stop measuring time here, and print it\n",
        "  auto t2 = std::chrono::high_resolution_clock::now();\n",
        "  auto duration = std::chrono::duration_cast<std::chrono::microseconds>( t2 - t1 ).count(); \n",
        "  std::cout << \"Time elapsed for CPU execution is \" << duration << \"us\" << std::endl;\n",
        "\n",
        "\n",
        "  // Check results\n",
        "  bool ok = checkResults(h_rgba, h_brg, WIDTH*HEIGHT);\n",
        "  if (ok) {\n",
        "      std::cout << \"Executed!! Results OK.\" << std::endl;\n",
        "  } else {\n",
        "      std::cout << \"Executed!! Results NOT OK.\" << std::endl;\n",
        "  }\n",
        "\n",
        "  // Destroy cuda stream here\n",
        "  cudaStreamDestroy(stream);\n",
        "\n",
        "  // Free CPU pointers\n",
        "  free(h_rgba);\n",
        "  free(h_brg);\n",
        "\n",
        "  // Free cuda pointers\n",
        "  freeCUDAPointers(d_brg, d_rgba);\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing done in this code cell is the stream creation. Here we can notice that we added the cudaStreamNonBlocking flag in order to prevent blocks while executing the code. From the Nvidia web we see that the flag \"Specifies that work running in the created stream may run concurrently with work in stream 0 (the NULL stream), and that the created stream should perform no implicit synchronization with stream 0.\"\n",
        "\n",
        "The next thing we notice is that in the call executeKernelconvertBRG2RGBA we added the stream at the end so that the function uses the stream that we have created. The next change that we see is that we use the stream when passing the data from the GPU to the CPU.\n",
        "\n",
        "Later, we see added the stream synchronize before stopping the timer as we have ended all the operations that we needed with the streams. \n",
        "\n",
        "After all the results have been checked we ended the code by destroying the streams as they're not needed anymore."
      ],
      "metadata": {
        "id": "uJsB7QHOd6ne"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Cb3WvwUW6E7"
      },
      "source": [
        "Do the following:\n",
        "\n",
        "1.   Use the fastest kernel version.\n",
        "2.   Use number of iterations = 1.\n",
        "3.   Compare the same kernel, with the original Host code, and this new Host code.\n",
        "4.   To do so, you can use the code you do now, you only need to set stream=0 in order to simulate the original code.\n",
        "5.   Execute it with the following code.\n",
        "6.   Compare and try to explain the performance difference in the report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icJq5yuCW9f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6804f44f-6539-4b3e-ee77-72c960971ba0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "convertBRG2RGBA time for 1 iterations = 1258us\n",
            "Time elapsed for CPU execution is 25454us\n",
            "First position x=0 y=0 z=255 w=255\n",
            "Executed!! Results OK.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cu\n",
        "#include \"/content/src/experiment.h\"\n",
        "int main() {\n",
        "\n",
        "  executeExperiment();\n",
        "\n",
        "  return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With new Host code and the best Kernel, that is the last kernel cell we have gotten a: \n",
        "\n",
        "convertBRG2RGBA time for 1 iterations = 1244us\n",
        "\n",
        "Time elapsed for CPU execution is 25582us\n",
        "\n",
        "Meanwhile, changing the stream values to 0 we got:\n",
        "\n",
        "convertBRG2RGBA time for 1 iterations = 1276us\n",
        "\n",
        "Time elapsed for CPU execution is 25049us\n",
        "\n",
        "\n",
        "We can see that we're not getting an improvement. That can be cause by the fact that the use of the streams should be done when we can overlap other operations such as launching new kernels from other streams. But, as we are only using a single stream and there's no other process in the CPU using another stream, the calls to the operations will be sequential. Just as if we had done them with the default stream in the original Host code."
      ],
      "metadata": {
        "id": "9yr1ISbNgXq-"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CUDALab_2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}