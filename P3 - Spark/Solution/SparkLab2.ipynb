{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkLab2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVryd47Toj3B"
      },
      "source": [
        "# PySpark Lab2\n",
        "\n",
        "In this lab we will see and test some more functionality of Spark.\n",
        "\n",
        "As in the previous lab, we start the notebook by installing pyspark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_t7OxPVkNtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6302b508-4af9-426f-9e73-23ec40c90f9d"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 54.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=85280310a5f58f827623188afccbf45c6eafbcdbaa517a8810699dbff37e0caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67mX47PSp59i"
      },
      "source": [
        "## Get the dataset\n",
        "\n",
        "In order to have a fast way to get the dataset we have prepared for this lab, we created a link to a file containing it in another google account, and written down all the necessary steps to get the file in the current path.\n",
        "\n",
        "This file is 2007.csv, and contains information about flights during the year 2007.\n",
        "\n",
        "Now, execute the following code cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQaMx5sxvIto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a08eccbb-7584-4b46-e82e-7f2ece331192"
      },
      "source": [
        "!gdown --id \"1QJ-wDWTc3oM_jbSlb5cB3HPJ7Jwy5iAH\"\n",
        "!unzip SparkTutorials2i3.zip\n",
        "!mv Spark_Tutorial2/2006.csv Spark_Tutorial2/2007.csv Spark_Tutorial2/2008.csv .\n",
        "!rm -r __MACOSX/ Spark_Tutorial3/ Spark_Tutorial2 SparkTutorials2i3.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QJ-wDWTc3oM_jbSlb5cB3HPJ7Jwy5iAH\n",
            "To: /content/SparkTutorials2i3.zip\n",
            "100% 887M/887M [00:03<00:00, 245MB/s]\n",
            "Archive:  SparkTutorials2i3.zip\n",
            "   creating: Spark_Tutorial2/\n",
            "  inflating: Spark_Tutorial2/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/Spark_Tutorial2/\n",
            "  inflating: __MACOSX/Spark_Tutorial2/._.DS_Store  \n",
            "  inflating: Spark_Tutorial2/2008.csv  \n",
            "  inflating: __MACOSX/Spark_Tutorial2/._2008.csv  \n",
            "  inflating: Spark_Tutorial2/2007.csv  \n",
            "  inflating: __MACOSX/Spark_Tutorial2/._2007.csv  \n",
            "  inflating: Spark_Tutorial2/2006.csv  \n",
            "  inflating: __MACOSX/Spark_Tutorial2/._2006.csv  \n",
            "  inflating: Spark_Tutorial2/tutorial2.pdf  \n",
            "  inflating: __MACOSX/Spark_Tutorial2/._tutorial2.pdf  \n",
            "  inflating: __MACOSX/._Spark_Tutorial2  \n",
            "   creating: Spark_Tutorial3/\n",
            "  inflating: Spark_Tutorial3/Tutorial3.zip  \n",
            "   creating: __MACOSX/Spark_Tutorial3/\n",
            "  inflating: __MACOSX/Spark_Tutorial3/._Tutorial3.zip  \n",
            "  inflating: Spark_Tutorial3/.DS_Store  \n",
            "  inflating: __MACOSX/Spark_Tutorial3/._.DS_Store  \n",
            "  inflating: Spark_Tutorial3/graphframes-0.5.0-spark2.1-s_2.11.jar  \n",
            "  inflating: __MACOSX/Spark_Tutorial3/._graphframes-0.5.0-spark2.1-s_2.11.jar  \n",
            "  inflating: Spark_Tutorial3/2007.csv  \n",
            "  inflating: __MACOSX/Spark_Tutorial3/._2007.csv  \n",
            "  inflating: Spark_Tutorial3/tutorial3.pdf  \n",
            "  inflating: __MACOSX/Spark_Tutorial3/._tutorial3.pdf  \n",
            "  inflating: __MACOSX/._Spark_Tutorial3  \n",
            "2006.csv  2007.csv  2008.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwY3OIb-5TfR"
      },
      "source": [
        "## Map Reduce in Spark\n",
        "\n",
        "For this lab we will be using three files. Let's load them!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUcg6USv37DY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b530f1e4-d08e-4bbb-a233-3ab5132bbabd"
      },
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[1]\").appName('testSparkSession').getOrCreate()\n",
        "\n",
        "df2006 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2006.csv\")\n",
        "df2007 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2007.csv\")\n",
        "df2008 = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"nullValue\",\"NA\").option(\"inferSchema\", \"true\").load(\"2008.csv\")\n",
        "\n",
        "print (\"df2006 number of partitions\", df2006.rdd.getNumPartitions())\n",
        "print (\"df2007 number of partitions\", df2007.rdd.getNumPartitions())\n",
        "print (\"df2008 number of partitions\", df2008.rdd.getNumPartitions())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df2006 number of partitions 6\n",
            "df2007 number of partitions 6\n",
            "df2008 number of partitions 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74y4TJdVGZMM"
      },
      "source": [
        "Now we loaded the three files and checked the number of partitions for each of them.\n",
        "\n",
        "Let's check the number of elements of each dataframe too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j3ixQKnGYC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53acf69c-e665-4ef3-f897-3e8a147c27d4"
      },
      "source": [
        "print (\"df2006 number of elements\", df2006.count())\n",
        "print (\"df2007 number of elements\", df2007.count())\n",
        "print (\"df2008 number of elements\", df2008.count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df2006 number of elements 7141922\n",
            "df2007 number of elements 7453215\n",
            "df2008 number of elements 7009728\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-r6hwUKG8S1"
      },
      "source": [
        "Let's now unify all data frames into one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji3M8IL0G_HM"
      },
      "source": [
        "df1 = df2006.union(df2007).union(df2008)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P5P4JDoRlKs"
      },
      "source": [
        "How many elements?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsbvuhiNRlWt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fcd8d4a-67cd-4b3f-feb8-70cae1e9dc92"
      },
      "source": [
        "df1.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21604865"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOL8lTspRltt"
      },
      "source": [
        "How many partitions?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzug6JXIRl-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff5bd00-a376-4070-aa1c-33ae81d818d0"
      },
      "source": [
        "df1.rdd.getNumPartitions()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYKBLZBwHDPe"
      },
      "source": [
        "Let's now do some filterting.\n",
        "\n",
        "First we pick some columns, and remove the na values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzn5Wp_HDiG"
      },
      "source": [
        "df2 = df1.select(\"Year\", \"Month\", \"Origin\", \"Dest\", \"ArrDelay\", \"DepDelay\")\n",
        "df3 = df2.na.drop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJuH-Mh4IVgs"
      },
      "source": [
        "Now, as in the other lab we compute the sum of arrival and departure delays, and store it in a new column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJjZ3NuDIWDj"
      },
      "source": [
        "from pyspark.sql.functions import expr\n",
        "df4 = df3.withColumn(\"SumDelay\", expr(\"ArrDelay + DepDelay\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TwviX7jIYzE"
      },
      "source": [
        "Again, we will use the cache functionality, to execute faster from this point"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38dU-IP0IZCx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e832f92-ff85-4b21-efcc-f78bc32c96e3"
      },
      "source": [
        "df4.cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Year: int, Month: int, Origin: string, Dest: string, ArrDelay: int, DepDelay: int, SumDelay: int]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BSWcwi3IbZd"
      },
      "source": [
        "Let's use grouping operations, to get for instance the averafe SumDelay for each Origin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDIGYMXQIb64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47e8228a-e364-475b-fb54-0b7b75627120"
      },
      "source": [
        "from pyspark.sql.functions import avg\n",
        "df5 = df4.groupBy(\"Origin\").agg(avg(\"SumDelay\"))\n",
        "df5.show()\n",
        "df5.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|Origin|      avg(SumDelay)|\n",
            "+------+-------------------+\n",
            "|   BGM| 14.275419982316533|\n",
            "|   DLG| 29.520242914979757|\n",
            "|   PSE| 1.6995652173913043|\n",
            "|   MSY| 16.297211527465407|\n",
            "|   GEG|   8.89446637700977|\n",
            "|   BUR| 12.278206798656793|\n",
            "|   SNA| 10.670062286517982|\n",
            "|   GRB|  20.25246703344686|\n",
            "|   GTF|-1.4841212989493793|\n",
            "|   IDA|   2.70422379478107|\n",
            "|   GRR| 15.865221292490036|\n",
            "|   EUG| 11.089682627446644|\n",
            "|   PSG|  22.49418604651163|\n",
            "|   MYR| 19.363025389374865|\n",
            "|   PVD|  16.99863997911887|\n",
            "|   GSO| 21.790378090751545|\n",
            "|   ISO|  34.06303724928367|\n",
            "|   OAK|  12.94959132626464|\n",
            "|   COD| -6.474365750528541|\n",
            "|   MSN| 20.945062622916883|\n",
            "+------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "315"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_xRayVxZQda"
      },
      "source": [
        "We can also rename a column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIhRM75_ZNuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664bb0d3-111d-4861-aa9c-fd06f293ebe5"
      },
      "source": [
        "df6 = df5.withColumnRenamed(\"avg(SumDelay)\", \"Average Delay\")\n",
        "df6.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------------------+\n",
            "|Origin|      Average Delay|\n",
            "+------+-------------------+\n",
            "|   BGM| 14.275419982316533|\n",
            "|   DLG| 29.520242914979757|\n",
            "|   PSE| 1.6995652173913043|\n",
            "|   MSY| 16.297211527465407|\n",
            "|   GEG|   8.89446637700977|\n",
            "|   BUR| 12.278206798656793|\n",
            "|   SNA| 10.670062286517982|\n",
            "|   GRB|  20.25246703344686|\n",
            "|   GTF|-1.4841212989493793|\n",
            "|   IDA|   2.70422379478107|\n",
            "|   GRR| 15.865221292490036|\n",
            "|   EUG| 11.089682627446644|\n",
            "|   PSG|  22.49418604651163|\n",
            "|   MYR| 19.363025389374865|\n",
            "|   PVD|  16.99863997911887|\n",
            "|   GSO| 21.790378090751545|\n",
            "|   ISO|  34.06303724928367|\n",
            "|   OAK|  12.94959132626464|\n",
            "|   COD| -6.474365750528541|\n",
            "|   MSN| 20.945062622916883|\n",
            "+------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}